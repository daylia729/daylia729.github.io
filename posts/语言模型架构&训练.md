---
title: 语言模型架构&训练
description: 语言模型架构&训练
date: 2025-09-26
tags:
  - cs336
---

### Transformer组件的各种变体
Difference:
* LayerNorm is in the front of the block
* Rotary position embeddings(RoPE)
* FF layers use SwiGLU,not ReLU
* Linear layers(and layernorm) have no bias(constant)terms

#### Pre norm is better than post norm
* Original transformer:LayerNorm after residual stream.First you do Multi-Head attention , add back to the resdual stream ,and then layernorm.You will do the same thing with the FFN layer.
* Use post norm is much less stable,which means you have to do some careful learning rate warm-up style things(reference the previous large model fine_tuning I did) to make it train in a stable way
* Now the layernorm is in front of the residual stream,which did much better in many different ways.(almost all modern LMs use Pre norm)
* Using pre norm we can remove warm up and it works just as well as post norm with some other stability-inducing tricks,even better
* Pre norm iss just a more stable architecture to tain.Today pre norm and other layer norm tricks being used essentially as stability-inducing
* New-things:double norm(pre norm with post norm) 
##### Why is layer norm in the residual bad?
* Residual gives identity connection all the way from almost the top of the network all the way to the bottom,this makes gradient propagation very easy when you train models,if putting layer norm in the middle,might mess with that kind of gradient behavior.(梯度在深层反向传播，容易造成梯度消失)
#### LayerNorm vs RMSNorm
* LayerNorm
$$
y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
$$
* RMSNorm
$$
y = \frac{x}{\sqrt{\|x\|_2^2 + \epsilon}} * \gamma
$$
* Many models have moved on to RSMNorm,which is faster and just as good as LayerNorm
* RMSNorm has fewer operations and fewer parameters
* Flops:Tensor constraction is 99.8%,Stat.normalization is 0.17% ,but for runtime,Tensor constraction is 61%,Stat.normalization is 25.5%.Because tensor constraction is dense computation,Stat.normalization operations incur a lot of memory movement overhead.
* We not only think about FLOPs,but also memory movement.Optimize this is matter.
#### More generally:dropping bias terms
* Most modern transformers don't have bias terms,it performs as well as before and makes train more stable
#### Activations
