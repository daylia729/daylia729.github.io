---
title: 大模型预训练
description: 大模型预训练
date: 2025-10-18
tags:
  - 大模型
---

### 一些需要知道的代码的基础知识
#### 1.确保项目可以找到项目中的所有模块
* `os.path.dirname(_file_)`获取当前脚本所在路径
* `os.path.join(...,...)`将两个路径拼接起来
* `os.path.abspath()`将相对路径转换为绝对路径
* `sys.path.append()`将路径添加到模块搜索路径中
* `sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))`就是将项目根目录添加到Python查找模块的路径列表中

### 因果语言建模(Causal Language Modeling)
* 让大模型学会接龙
* 原理：
```
原始序列: [token1, token2, token3, token4, ..., token_n]
输入X:    [token1, token2, token3, ..., token_{n-1}]
目标Y:    [token2, token3, token4, ..., token_n]

训练目标: 给定X预测Y，即根据前文预测下一个token
```

### 预训练流程
#### 以minimind为例 一般包括以下几个步骤
1.数据准备和预处理 
* 在dataset/lm_dataset.py中实现的PretrainDataset类实现具体的数据处理逻辑，然后import这个类 
* 首先原始文本经过分词之后得到token ID向量，然后根据max_length的值，不够就填充0，长的就截断，损失掩码形状和填充后的一样，真实token对应是1，填充的对应是0；输入的序列x去掉最后一个token，目标序列y去掉第一个token，损失掩码与y对齐，只计算真实token的损失
```
class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)

    def load_data(self, path):
        samples = []
        with open(path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                data = json.loads(line.strip())
                samples.append(data)
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        sample = self.samples[index]

        # 构建输入文本
        encoding = self.tokenizer(
            str(sample['text']),   #将文本转化为字符串
            max_length=self.max_length,
            padding='max_length',
            truncation=True,       #超长文本截断
            return_tensors='pt'    #返回Pytorch tensor
        )
        input_ids = encoding.input_ids.squeeze()   #去除batch维度
        loss_mask = (input_ids != self.tokenizer.pad_token_id)  #损失掩码

        X = torch.tensor(input_ids[:-1], dtype=torch.long)
        Y = torch.tensor(input_ids[1:], dtype=torch.long)
        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)
        return X, Y, loss_mask
```
* 返回的encoding是一个字典，其中的input_ids是一个形状为(1,seq_len)的张量，调用.squeeeze()方法，降低一个维度去掉，得到(seq_len),DataLoader会自动将这些一维张量堆叠成二维张量（batch_size, seq_len）
```
tensor([[101,102,103,104,0,0]])
```
```
tensor([101,102,103,104,0,0])
```
2.分词器
* .from_pretrained()方法可以直接调用一个现成的或者自定义的预训练资源
3.模型架构和配置
```
def init_model(lm_config):
    tokenizer = AutoTokenizer.from_pretrained('../model/')
    model = MiniMindForCausalLM(lm_config).to(args.device)
    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')
    return model, tokenizer
```



