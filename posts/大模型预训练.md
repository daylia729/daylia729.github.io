---
title: 大模型预训练
description: 大模型预训练
date: 2025-10-18
tags:
  - 大模型
---

### 一些需要知道的代码的基础知识
#### 1.确保项目可以找到项目中的所有模块
* `os.path.dirname(_file_)`获取当前脚本所在路径
* `os.path.join(...,...)`将两个路径拼接起来
* `os.path.abspath()`将相对路径转换为绝对路径
* `sys.path.append()`将路径添加到模块搜索路径中
* `sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))`就是将项目根目录添加到Python查找模块的路径列表中

#### 因果语言建模(Causal Language Modeling)
* 让大模型学会接龙
* 原理：
```
原始序列: [token1, token2, token3, token4, ..., token_n]
输入X:    [token1, token2, token3, ..., token_{n-1}]
目标Y:    [token2, token3, token4, ..., token_n]

训练目标: 给定X预测Y，即根据前文预测下一个token
```
#### 交叉熵损失计算的一些细节
* 首先我们有经过大模型处理后得到的logits，以及真实的标签y，计算损失时，将logits的形状由(batch_size,seq_len,vocab_size)变成(batch_size*seq__len,vocab_size)，y的形状由(batch_size*seq_len)
* 然后对logits进行log softmax计算，根据真实标签y取出对应位置的概率（都小于1），交叉熵损失就是这些负对数概率的平均值或和（取决于reduction参数）
#### 学习率
* `lr/10` 保证即使后面余弦的部分衰减到0学习率不会低于这个数，防止学习率过小导致训练停滞
* 余弦退火  `0.5*lr*(1+math.cos(math.pi*current_step/total_steps))`保证模型训练在初期可以快速学习，中间稳定收敛，后期精细调整
#### 上下文管理器 contextlib
* 实现混合精度训练，`ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast()`  cpu不支持AMP
* 实现禁用梯度计算,`ctx = torch.no_grad`
* 推理模式 `torch.inference_model`
* ...
#### 混合精度训练 scaler
* 梯度缩放器 scaler why? 由于FP16比FP32表示的范围小，以至于在FP16中变成0，为了避免，我们使用一个缩放因子来放大损失值，从而在反向传播中放大梯度，使其在FP16的表示范围内，然后在优化器更新参数前，再讲梯度缩小为原来的尺度
* `scaler.scale(loss).backward`将前向传播得到的loss乘以一个`scaler factor`，保证使用FP16时不会发生下溢；然后`scaler.unscale_(optimizer)`,`scaler.step(optimizer)`会更新参数，如果有nan或者inf会自动跳过；最后`scaler.update()`来更新`scaler factor`
### 预训练流程
#### 以minimind为例 一般包括以下几个步骤
1.数据准备和预处理 
* 在dataset/lm_dataset.py中实现的PretrainDataset类实现具体的数据处理逻辑，然后import这个类 
* 首先原始文本经过分词之后得到token ID向量，然后根据max_length的值，不够就填充0，长的就截断，损失掩码形状和填充后的一样，真实token对应是1，填充的对应是0；输入的序列x去掉最后一个token，目标序列y去掉第一个token，损失掩码与y对齐，只计算真实token的损失
```
class PretrainDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(data_path)

    def load_data(self, path):
        samples = []
        with open(path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                data = json.loads(line.strip())
                samples.append(data)
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, index):
        sample = self.samples[index]

        # 构建输入文本
        encoding = self.tokenizer(
            str(sample['text']),   #将文本转化为字符串
            max_length=self.max_length,
            padding='max_length',
            truncation=True,       #超长文本截断
            return_tensors='pt'    #返回Pytorch tensor
        )
        input_ids = encoding.input_ids.squeeze()   #去除batch维度
        loss_mask = (input_ids != self.tokenizer.pad_token_id)  #损失掩码

        X = torch.tensor(input_ids[:-1], dtype=torch.long)
        Y = torch.tensor(input_ids[1:], dtype=torch.long)
        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)
        return X, Y, loss_mask
```
* 返回的encoding是一个字典，其中的input_ids是一个形状为(1,seq_len)的张量，调用.squeeeze()方法，降低一个维度去掉，得到(seq_len),DataLoader会自动将这些一维张量堆叠成二维张量（batch_size, seq_len）
```
tensor([[101,102,103,104,0,0]])
```
```
tensor([101,102,103,104,0,0])
```
2.分词器
* .from_pretrained()方法可以直接调用一个现成的或者自定义的预训练资源
3.模型架构和配置
```
def init_model(lm_config):
    tokenizer = AutoTokenizer.from_pretrained('../model/')
    model = MiniMindForCausalLM(lm_config).to(args.device)
    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')
    return model, tokenizer
```


### FileZilla
今天上传预训练数据，总是因为网络不稳定以及文件太大失败，然后下载了FileZilla，传的依然很慢，但是优点是可以从断点接着上传